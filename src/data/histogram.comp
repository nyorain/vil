#version 460

#extension GL_GOOGLE_include_directive : require

// layout(local_size_x_id = 0, local_size_y_id = 1) in;
layout(local_size_x = 16u, local_size_y = 16u) in;

#define SAMPLE_TEX_BINDING 1
#include "sample.glsl"
#include "histogram.glsl"

layout(push_constant) uniform PCR {
	int level;
	int layer;
	// NOTE: for a full histogram this would be 1.
	// For larger values, we effectively only sample every nth texe. 
	// Useful as optimization on larger images.
	int texelSkip;
} pcr;

layout(set = 0, binding = 0) buffer Hist {
	HistMetadata meta;
	uvec4 data[];
} hist;

// TODO: could speed this up with subgroup ballot or groupshared mem
//   are atomics faster on groupshared? probably
//   subgroup ballot only works for few samples, we don't want a *whole*
//   histogram per thread (would crush VGPR pressure).
// shared uvec4 data[256]; // max histogram size

const uint sampleSize = 2u;

void process(ivec3 coords) {
	if(!coordsInside(coords, pcr.level)) {
		return;
	}

	vec4 col = fetchTex(coords, pcr.level);

	float range = hist.meta.end - hist.meta.begin;

	const bool ignoreBounds = false;

	int len = hist.data.length();
	for(uint i = 0u; i < 4; ++i) {
		if(ignoreBounds && (
				col[i] == hist.meta.begin ||
				col[i] == hist.meta.end)) {
			continue;
		}

		float pos = (col[i] - hist.meta.begin) / range;
		// uint id = clamp(pos * len, 0, len - 1);
		// NOTE: we need the floor here, don't want trunc
		int id = int(floor(pos * len));
		if(id >= 0 && id < len) {
			atomicAdd(hist.data[id][i], 1u);
		}
	}
}

void main() {
	ivec3 coords = ivec3(sampleSize * gl_GlobalInvocationID.xyz);
	coords *= pcr.texelSkip;
	coords.z += pcr.layer;

	for(uint y = 0u; y < sampleSize; ++y) {
		for(uint x = 0u; x < sampleSize; ++x) {
			ivec3 lcoords = coords;
			lcoords.xy += pcr.texelSkip * ivec2(x, y);
			process(coords);
		}
	}
}
